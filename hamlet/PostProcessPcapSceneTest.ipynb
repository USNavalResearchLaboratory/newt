{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This Notebook is an example of using pcap pandas analytics to build a temporal graph from Ian Taylor's Newt orchestration of Hamlet by Scenes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import IPython\n",
    "import matplotlib\n",
    "from distutils.version import StrictVersion\n",
    "ipy4 = False\n",
    "violin = False\n",
    "if StrictVersion(IPython.__version__) >= StrictVersion(\"4.0\"):\n",
    "    ipy4 = True\n",
    "if StrictVersion(matplotlib.__version__) >= StrictVersion(\"1.5\"):\n",
    "    violin = True\n",
    "    \n",
    "import log2pd as lpd\n",
    "#import plotpd as ppd\n",
    "import pandas as pd\n",
    "import log2plot as l2plot\n",
    "import analystPlots as appd\n",
    "import logAnalyticsLib as lalib\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cmx\n",
    "import numpy as np\n",
    "import os\n",
    "import warnings\n",
    "from netaddr import IPNetwork\n",
    "import glob\n",
    "from StringIO import StringIO\n",
    "import fileinput\n",
    "from pandas.lib import Timestamp\n",
    "import datetime\n",
    "import subprocess\n",
    "#from IPython.display import Image\n",
    "#os.system(\"taskset -p 0xFFFFFFFF %d\" % os.getpid())\n",
    "# Setup some plot defaults for the duration of this notebook\n",
    "#ppd.setup_plot_defaults()\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "root_dir = '.'\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#df = lalib.load_hdf5('./store-mgen-mcast-ham.hdf5')  \n",
    "from scipy.interpolate import interp1d\n",
    "from operator import itemgetter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "json_data=open(\"./data/core_actor_mapping.json\").read()\n",
    "d = json.loads(json_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{u'10.0.0.33': u'n33', u'10.0.0.32': u'n32', u'10.0.0.31': u'n31', u'10.0.0.30': u'n30', u'10.0.0.34': u'n34', u'10.0.0.15': u'n15', u'10.0.0.14': u'n14', u'10.0.0.17': u'n17', u'10.0.0.16': u'n16', u'10.0.0.11': u'n11', u'10.0.0.10': u'n10', u'10.0.0.13': u'n13', u'10.0.0.12': u'n12', u'10.0.0.19': u'n19', u'10.0.0.18': u'n18', u'10.0.0.9': u'n9', u'10.0.0.8': u'n8', u'10.0.0.5': u'n5', u'10.0.0.4': u'n4', u'10.0.0.7': u'n7', u'10.0.0.6': u'n6', u'10.0.0.1': u'n1', u'10.0.0.3': u'n3', u'10.0.0.2': u'n2', u'10.0.0.28': u'n28', u'10.0.0.29': u'n29', u'10.0.0.24': u'n24', u'10.0.0.25': u'n25', u'10.0.0.26': u'n26', u'10.0.0.27': u'n27', u'10.0.0.20': u'n20', u'10.0.0.21': u'n21', u'10.0.0.22': u'n22', u'10.0.0.23': u'n23'}\n",
      "{u'n12': 0, u'n13': 0, u'n10': 0, u'n11': 0, u'n16': 0, u'n17': 0, u'n14': 0, u'n15': 0, u'n18': 0, u'n19': 0, u'n30': 0, u'n31': 0, u'n32': 0, u'n33': 0, u'n34': 0, u'n29': 0, u'n28': 0, u'n23': 0, u'n22': 0, u'n21': 0, u'n20': 0, u'n27': 0, u'n26': 0, u'n25': 0, u'n24': 0, u'n8': 0, u'n9': 0, u'n1': 0, u'n2': 0, u'n3': 0, u'n4': 0, u'n5': 0, u'n6': 0, u'n7': 0}\n",
      "[(u'n12', {'weight': 0}), (u'n13', {'weight': 0}), (u'n10', {'weight': 0}), (u'n11', {'weight': 0}), (u'n16', {'weight': 0}), (u'n17', {'weight': 0}), (u'n14', {'weight': 0}), (u'n15', {'weight': 0}), (u'n18', {'weight': 0}), (u'n19', {'weight': 0}), (u'n30', {'weight': 0}), (u'n31', {'weight': 0}), (u'n32', {'weight': 0}), (u'n33', {'weight': 0}), (u'n34', {'weight': 0}), (u'n29', {'weight': 0}), (u'n28', {'weight': 0}), (u'n23', {'weight': 0}), (u'n22', {'weight': 0}), (u'n21', {'weight': 0}), (u'n20', {'weight': 0}), (u'n27', {'weight': 0}), (u'n26', {'weight': 0}), (u'n25', {'weight': 0}), (u'n24', {'weight': 0}), (u'n8', {'weight': 0}), (u'n9', {'weight': 0}), (u'n1', {'weight': 0}), (u'n2', {'weight': 0}), (u'n3', {'weight': 0}), (u'n4', {'weight': 0}), (u'n5', {'weight': 0}), (u'n6', {'weight': 0}), (u'n7', {'weight': 0})]\n"
     ]
    }
   ],
   "source": [
    "# Read the node file and create a dict of ip address to CORE nodename\n",
    "n_to_a_dict={}\n",
    "for actor,value in d.iteritems():\n",
    "    n_to_a_dict[value['address']]=value['node']\n",
    "print n_to_a_dict\n",
    "#n_to_a_dict['224.0.5.2']='n50'\n",
    "#\n",
    "# Use CORE names for graph nodes\n",
    "\n",
    "import networkx as nx\n",
    "G=nx.DiGraph()\n",
    "nodes = n_to_a_dict.values()\n",
    "G.add_nodes_from(nodes)\n",
    "n_weights={node:0 for node in nodes}\n",
    "print n_weights\n",
    "nx.set_node_attributes(G,'weight',n_weights)\n",
    "print G.nodes(data=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "node_to_actor={}\n",
    "for actor,value in d.iteritems():\n",
    "    node_to_actor[value['node']]=actor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load_pcap_data() Error: pcap nodups file in dataset, file ./results/Collect_ActIIISceneIII//n4/n4.eth0.pcap.nodups not processed.\n",
      "load_pcap_data() Error: pcap nodups file in dataset, file ./results/Collect_ActIIISceneIII//n30/n30.eth0.pcap.nodups not processed.\n",
      "load_pcap_data() Error: pcap nodups file in dataset, file ./results/Collect_ActIIISceneIII//n20/n20.eth0.pcap.nodups not processed.\n",
      "load_pcap_data() Error: pcap nodups file in dataset, file ./results/Collect_ActIIISceneIII//n33/n33.eth0.pcap.nodups not processed.\n",
      "load_pcap_data() Error: pcap nodups file in dataset, file ./results/Collect_ActIIISceneIII//n29/n29.eth0.pcap.nodups not processed.\n",
      "load_pcap_data() Error: pcap nodups file in dataset, file ./results/Collect_ActIIISceneIII//n14/n14.eth0.pcap.nodups not processed.\n",
      "load_pcap_data() Error: pcap nodups file in dataset, file ./results/Collect_ActIIISceneIII//n32/n32.eth0.pcap.nodups not processed.\n",
      "load_pcap_data() Error: pcap nodups file in dataset, file ./results/Collect_ActIIISceneIII//n11/n11.eth0.pcap.nodups not processed.\n",
      "load_pcap_data() Error: pcap nodups file in dataset, file ./results/Collect_ActIIISceneIII//n8/n8.eth0.pcap.nodups not processed.\n",
      "load_pcap_data() Error: pcap nodups file in dataset, file ./results/Collect_ActIIISceneIII//n17/n17.eth0.pcap.nodups not processed.\n",
      "load_pcap_data() Error: pcap nodups file in dataset, file ./results/Collect_ActIIISceneIII//n27/n27.eth0.pcap.nodups not processed.\n",
      "load_pcap_data() Error: pcap nodups file in dataset, file ./results/Collect_ActIIISceneIII//n1/n1.eth0.pcap.nodups not processed.\n",
      "load_pcap_data() Error: pcap nodups file in dataset, file ./results/Collect_ActIIISceneIII//n19/n19.eth0.pcap.nodups not processed.\n",
      "load_pcap_data() Error: pcap nodups file in dataset, file ./results/Collect_ActIIISceneIII//n28/n28.eth0.pcap.nodups not processed.\n",
      "load_pcap_data() Error: pcap nodups file in dataset, file ./results/Collect_ActIIISceneIII//n22/n22.eth0.pcap.nodups not processed.\n",
      "load_pcap_data() Error: pcap nodups file in dataset, file ./results/Collect_ActIIISceneIII//n26/n26.eth0.pcap.nodups not processed.\n",
      "load_pcap_data() Error: pcap nodups file in dataset, file ./results/Collect_ActIIISceneIII//n25/n25.eth0.pcap.nodups not processed.\n",
      "load_pcap_data() Error: pcap nodups file in dataset, file ./results/Collect_ActIIISceneIII//n5/n5.eth0.pcap.nodups not processed.\n",
      "load_pcap_data() Error: pcap nodups file in dataset, file ./results/Collect_ActIIISceneIII//n23/n23.eth0.pcap.nodups not processed.\n",
      "load_pcap_data() Error: pcap nodups file in dataset, file ./results/Collect_ActIIISceneIII//n16/n16.eth0.pcap.nodups not processed.\n",
      "load_pcap_data() Error: pcap nodups file in dataset, file ./results/Collect_ActIIISceneIII//n34/n34.eth0.pcap.nodups not processed.\n",
      "load_pcap_data() Error: pcap nodups file in dataset, file ./results/Collect_ActIIISceneIII//n13/n13.eth0.pcap.nodups not processed.\n",
      "load_pcap_data() Error: pcap nodups file in dataset, file ./results/Collect_ActIIISceneIII//n31/n31.eth0.pcap.nodups not processed.\n",
      "load_pcap_data() Error: pcap nodups file in dataset, file ./results/Collect_ActIIISceneIII//n15/n15.eth0.pcap.nodups not processed.\n",
      "load_pcap_data() Error: pcap nodups file in dataset, file ./results/Collect_ActIIISceneIII//n6/n6.eth0.pcap.nodups not processed.\n",
      "load_pcap_data() Error: pcap nodups file in dataset, file ./results/Collect_ActIIISceneIII//n12/n12.eth0.pcap.nodups not processed.\n",
      "load_pcap_data() Error: pcap nodups file in dataset, file ./results/Collect_ActIIISceneIII//n21/n21.eth0.pcap.nodups not processed.\n",
      "load_pcap_data() Error: pcap nodups file in dataset, file ./results/Collect_ActIIISceneIII//n2/n2.eth0.pcap.nodups not processed.\n",
      "load_pcap_data() Error: pcap nodups file in dataset, file ./results/Collect_ActIIISceneIII//n18/n18.eth0.pcap.nodups not processed.\n",
      "load_pcap_data() Error: pcap nodups file in dataset, file ./results/Collect_ActIIISceneIII//n9/n9.eth0.pcap.nodups not processed.\n",
      "load_pcap_data() Error: pcap nodups file in dataset, file ./results/Collect_ActIIISceneIII//n7/n7.eth0.pcap.nodups not processed.\n",
      "load_pcap_data() Error: pcap nodups file in dataset, file ./results/Collect_ActIIISceneIII//n10/n10.eth0.pcap.nodups not processed.\n",
      "load_pcap_data() Error: pcap nodups file in dataset, file ./results/Collect_ActIIISceneIII//n3/n3.eth0.pcap.nodups not processed.\n",
      "load_pcap_data() Error: pcap nodups file in dataset, file ./results/Collect_ActIIISceneIII//n24/n24.eth0.pcap.nodups not processed.\n",
      "time\n",
      "2016-04-29 14:16:33.908909     10.0.0.4\n",
      "2016-04-29 14:16:34.200996    10.0.0.14\n",
      "2016-04-29 14:16:34.407774    10.0.0.31\n",
      "2016-04-29 14:16:34.572853     10.0.0.4\n",
      "2016-04-29 14:16:34.737359    10.0.0.31\n",
      "2016-04-29 14:16:35.029087    10.0.0.27\n",
      "2016-04-29 14:16:35.192990     10.0.0.4\n",
      "2016-04-29 14:16:35.317019    10.0.0.15\n",
      "Name: ipSrc, dtype: object\n"
     ]
    }
   ],
   "source": [
    "H=G.copy()\n",
    "result='./results/Collect_ActIIISceneIII/'\n",
    "scenepd=lpd.load_pcap_data(result,\n",
    "                           duplicates=True,\n",
    "                           timeseries=True,\n",
    "                           dataSet=True)\n",
    "sorteddf=scenepd.set_index('time')\n",
    "sorteddf=sorteddf.sort_index()\n",
    "sorteddf=sorteddf[(sorteddf[\"frameType\"]==\"UDP\") & (sorteddf[\"dir\"]==2)]\n",
    "\n",
    "print sorteddf['ipSrc']\n",
    "###### Build and estimated  CAG from adjacencies\n",
    "last_speaker=None\n",
    "# Build a graph from multicast packets received\n",
    "# Use ipSrc information to create pair-wise adjacencies\n",
    "for index,row in sorteddf.iterrows():\n",
    "    if last_speaker == None:\n",
    "        last_speaker = row.ipSrc\n",
    "    elif last_speaker == row.ipSrc:\n",
    "        pass\n",
    "    else:\n",
    "        H.node[n_to_a_dict[row.ipSrc]][\"weight\"] += 1\n",
    "        if H.has_edge(n_to_a_dict[last_speaker],n_to_a_dict[row.ipSrc]):\n",
    "            H[n_to_a_dict[last_speaker]][n_to_a_dict[row.ipSrc]]['weight'] += 1\n",
    "        else:\n",
    "            H.add_edge(n_to_a_dict[last_speaker],n_to_a_dict[row.ipSrc],weight= 1)\n",
    "        last_speaker= row.ipSrc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('hamlet_stanzas.json', 'r') as handle:\n",
    "    stanza_dict = json.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18\n"
     ]
    }
   ],
   "source": [
    "actor1=\"Osric\"\n",
    "actor2=\"Hamlet\"\n",
    "count=0\n",
    "first=True\n",
    "for act in stanza_dict.iteritems():\n",
    "    for scene in act[1].iteritems():\n",
    "        stanzas=scene[1]\n",
    "        if first:\n",
    "            first=False\n",
    "        for stanza in stanzas.iteritems():\n",
    "            if stanza[1]['actor']==actor1 and stanza[1]['next_actor']==actor2:\n",
    "                count=count+1\n",
    "print count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
